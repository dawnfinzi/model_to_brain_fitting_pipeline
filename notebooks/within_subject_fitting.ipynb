{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fifty-outside",
   "metadata": {},
   "source": [
    "As a comparison to the subject to subject fitting, try using subject X trial 1 (for 1000 shared images) to predict subject X trial 2 etc. (should hopefully be similar to the split-half reliability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note on notebook: this did not work (i.e. regression performance way lower than expected). Not totally sure why. \n",
    "#Used it to save out organized split half vals instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "every-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import nibabel.freesurfer.mghformat as mgh\n",
    "import h5py\n",
    "import os\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import scipy.stats as stats\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brief-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "oakzfs_stem = '/sni-storage/kalanit/Projects/Dawn/NSD/'\n",
    "\n",
    "UTILS_PATH = oakzfs_stem + 'code/fit_pipeline/utils/'\n",
    "BETA_PATH = oakzfs_stem + 'local_data/processed/organized_betas/'\n",
    "STIM_PATH = oakzfs_stem + 'data/nsddata_stimuli/stimuli/nsd/'\n",
    "NSDDATA_PATH = oakzfs_stem + 'data/nsddata/'\n",
    "FS_PATH = oakzfs_stem + 'local_data/freesurfer/'\n",
    "FEATS_PATH = oakzfs_stem + 'code/fit_pipeline/models/features/'\n",
    "RESULTS_PATH = oakzfs_stem + 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smart-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(UTILS_PATH)\n",
    "\n",
    "from rsm_utils import get_ROI_data\n",
    "import regression_utils as rutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "younger-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times each image was shown\n",
    "N_REPEATS = 3\n",
    "\n",
    "# layer names for different models\n",
    "ALEXNET_LAYERS = ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc6', 'fc7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "valued-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = \"02\"\n",
    "hemi = \"rh\"\n",
    "roi = \"streams_shrink10\"\n",
    "sub_roi = \"ventral\"\n",
    "sub_roix = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "provincial-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(subj):\n",
    "    \n",
    "    order = scipy.io.loadmat(BETA_PATH + 'datab3nativesurface_subj' + subj)\n",
    "    data = pd.read_csv(NSDDATA_PATH + 'ppdata/subj' + subj + '/behav/responses.tsv', sep='\\t')\n",
    "    expdesign = scipy.io.loadmat(NSDDATA_PATH + 'experiments/nsd/nsd_expdesign.mat')\n",
    "    \n",
    "    #73KIDs\n",
    "    all_ids = np.array(data['73KID'])\n",
    "    vals, idx_start, count = np.unique(all_ids, return_counts=True, return_index=True)\n",
    "    which_reps = vals[count == N_REPEATS]\n",
    "    mask_3reps = np.isin(all_ids,which_reps)\n",
    "    id_nums_3reps = np.array(data['73KID'])[mask_3reps]\n",
    "    rep_vals = np.unique(id_nums_3reps) #sorted version of beta order\n",
    "    \n",
    "    #how the betas are ordered (using COCO 73K id numbers)\n",
    "    beta_order_in_73Kids = all_ids[order['allixs'][0]-1]-1 #-1 to convert from matlab to python indexing\n",
    "    \n",
    "    # shared (i.e. validation) IDS (but include all potential shared reps for the subj, not min across subjs)\n",
    "    sharedix = expdesign['sharedix'][0]\n",
    "    validation_mask = np.isin(rep_vals,sharedix)\n",
    "\n",
    "    return beta_order_in_73Kids, validation_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "threaded-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(oakzfs_stem + 'local_data/processed/' + 'rh_betas_by_repeat_by_ROI_zscore_1000.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    rh_betas_by_repeat_by_ROI = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "weird-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj02_rh_betas = rh_betas_by_repeat_by_ROI[1] #only subj2 (btw only includes subjs 1,2,5,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "twelve-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_half =  scipy.io.loadmat(FS_PATH + 'subj' + subj + '/rh_split_half.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "vietnamese-parent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 239309)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_half['mean'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "broadband-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's just load and reorganize all subjects split-half calcs by ROI while we're at it\n",
    "subjid = ['01', '02', '03', '04', '05', '06', '07', '08']\n",
    "ROI_names = ['Unknown', 'Early', 'Midventral', 'Midlateral', 'Midparietal', 'Ventral', 'Lateral', 'Parietal']\n",
    "\n",
    "split_half_by_subj = []\n",
    "for sidx, sid in enumerate(subjid):\n",
    "    sh =  scipy.io.loadmat(FS_PATH + 'subj' + sid + '/' + hemi + '_split_half.mat')\n",
    "    split_half_by_subj.append(sh['mean'])\n",
    "    \n",
    "streams = []\n",
    "for sidx, sid in enumerate(subjid):\n",
    "    mgh_file = mgh.load(FS_PATH + '/subj' + sid + '/' + hemi + '.streams.mgz')\n",
    "    streams.append(mgh_file.get_fdata()[:,0,0])\n",
    "    \n",
    "#organize split-half by ROI\n",
    "split_half_by_ROI = [[[] for j in range(len(ROI_names)-1)] for i in range(len(subjid))]\n",
    "\n",
    "#two loops because otherwise we run out of mems\n",
    "for sidx, sid in enumerate(subjid):  \n",
    "    for roi_idx in range(len(ROI_names)-1):       \n",
    "        split_half_by_ROI[sidx][roi_idx] = split_half_by_subj[sidx][:,streams[sidx] == roi_idx+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "optimum-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for future use\n",
    "with open(RESULTS_PATH + 'rh_split_half_by_ROI.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(split_half_by_ROI, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fossil-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_split_half_by_ROI = split_half_by_ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "technical-award",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 239309)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_half_by_subj[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "honey-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj02_sh_by_ROI = split_half_by_ROI[1] #for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "confused-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "ventral_mask = subj02_sh_by_ROI[4]>.1\n",
    "lateral_mask = subj02_sh_by_ROI[5]>.1\n",
    "parietal_mask = subj02_sh_by_ROI[6]>.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "major-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "ventral_trim = np.zeros((N_REPEATS, 1000, np.sum(ventral_mask[0])))\n",
    "ventral_trim[0,:,:] = subj02_rh_betas[4][0][:,ventral_mask[0]]\n",
    "ventral_trim[1,:,:] = subj02_rh_betas[4][1][:,ventral_mask[0]]\n",
    "ventral_trim[2,:,:] = subj02_rh_betas[4][2][:,ventral_mask[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "hired-innocent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1000, 9051)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ventral_trim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "valued-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all splits for regression training\n",
    "num_splits = 1 #average over two random splits\n",
    "all_splits = rutils.get_splits(data=ventral_trim,\n",
    "                               split_index=1,\n",
    "                               num_splits=num_splits,\n",
    "                               num_per_class_test = 200,\n",
    "                               num_per_class_train = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "banned-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 0\n",
      "evaluating 1\n",
      "evaluating 2\n"
     ]
    }
   ],
   "source": [
    "all_resdict = {}\n",
    "\n",
    "t1 = [0, 1, 2]\n",
    "t2 = [1, 2, 0]\n",
    "\n",
    "for s in range(N_REPEATS):   #for trial comb ...\n",
    "    \n",
    "    print('evaluating %s' % s)\n",
    "    feats = ventral_trim[s,:,:]\n",
    "\n",
    "    res = rutils.train_and_test_scikit_regressor(features=feats, \n",
    "                                                    labels=ventral_trim[t2[s],:,:],\n",
    "                                                    splits=all_splits,\n",
    "                                                    model_class=PLSRegression,\n",
    "                                                    model_args={'n_components': 5, #to match\n",
    "                                                                'scale': False},\n",
    "                                                    feature_norm=False)\n",
    "            \n",
    "    all_resdict[s] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "healthy-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquared_array = {}\n",
    "for s in range(N_REPEATS):\n",
    "    rsquared_array[s] = all_resdict[s]['test']['mean_rsquared_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "excess-calendar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10028541319595886"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rsquared_array[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "toxic-story",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 9051)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ventral_trim[c2[s],:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "amended-characterization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.224239060532011"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(subj02_sh_by_ROI[4][ventral_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "nominated-bruce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "occasional-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vox = np.sum(ventral_mask[0])\n",
    "\n",
    "#calculate split-half reliability\n",
    "corrvals = np.zeros((n_vox,3))\n",
    "for vox in range(n_vox):\n",
    "    for r in range(3):\n",
    "        corrval = stats.pearsonr(ventral_trim[t1[r],:,vox],\n",
    "                                ventral_trim[t2[r],:,vox])[0]\n",
    "        corrvals[vox, r] = corrval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "decreased-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_sh = np.mean(corrvals,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "superb-beginning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15812837, 0.21005189, 0.2294373 , ..., 0.10776668, 0.0785798 ,\n",
       "       0.08121873])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "induced-three",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17599837, 0.20001384, 0.20306887, ..., 0.11013285, 0.10445924,\n",
       "       0.1058643 ])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj02_sh_by_ROI[4][ventral_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "loved-discovery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9802131299290657"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.pearsonr(subset_sh,subj02_sh_by_ROI[4][ventral_mask])[0] #split half calc using all images vs shared 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "julian-content",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00654457,  0.05105688,  0.02149402, ...,  0.0902879 ,\n",
       "        0.10893416,  0.08424819])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsquared_array[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "social-westminster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/share/kalanit/biac2/kgs/projects/Dawn/NSD/code/fit_pipeline/notebooks'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "postal-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemi = \"lh\"\n",
    "split_half_by_subj = []\n",
    "for sidx, sid in enumerate(subjid):\n",
    "    sh =  scipy.io.loadmat(FS_PATH + 'subj' + sid + '/' + hemi + '_split_half.mat')\n",
    "    split_half_by_subj.append(sh['mean'])\n",
    "    \n",
    "streams = []\n",
    "for sidx, sid in enumerate(subjid):\n",
    "    mgh_file = mgh.load(FS_PATH + '/subj' + sid + '/' + hemi + '.streams.mgz')\n",
    "    streams.append(mgh_file.get_fdata()[:,0,0])\n",
    "    \n",
    "#organize split-half by ROI\n",
    "split_half_by_ROI = [[[] for j in range(len(ROI_names)-1)] for i in range(len(subjid))]\n",
    "\n",
    "#two loops because otherwise we run out of mems\n",
    "for sidx, sid in enumerate(subjid):  \n",
    "    for roi_idx in range(len(ROI_names)-1):       \n",
    "        split_half_by_ROI[sidx][roi_idx] = split_half_by_subj[sidx][:,streams[sidx] == roi_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "dutch-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for future use\n",
    "with open(RESULTS_PATH + 'lh_split_half_by_ROI.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(split_half_by_ROI, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-gallery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
